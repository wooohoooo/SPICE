{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7e8bd96-e094-401f-aaa8-51aa24381d04",
   "metadata": {},
   "source": [
    "# Bayesian Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd78c1-9730-4f26-b781-ae404d5570c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Bayesian Appendix — Beta–Binomial analyses for SPC, abuse detection, and adequacy.\n",
    "\n",
    "What this script does:\n",
    "- Loads the latest results CSV from the experiment runner.\n",
    "- Uses conjugate Beta–Binomial with Beta(1,1) priors.\n",
    "- Reports posterior means, 95% credible intervals (CrI), and Pr[pB > pA] for contrasts.\n",
    "- Covers:\n",
    "  (A) SPC by Tone (overall) + pairwise contrasts + Pr[friendly > unclear > abusive]\n",
    "  (B) SPC by Tone × Condition (all tones/conditions) + key contrasts (1b vs 1a and 2b vs 2a) overall and per model\n",
    "  (C) SPC by Model × Tone (cell posteriors)\n",
    "  (D) Abuse detection (YES) by Model × Tone; note false-negative / false-positive posteriors\n",
    "  (E) Adequacy (YES) by Model × Tone\n",
    "  (F) Abusive interactions: SPC posteriors when abuse is Recognized vs Missed (overall and per model)\n",
    "\n",
    "Outputs:\n",
    "- bayes_outputs/*.csv files with posterior summaries and contrast results.\n",
    "\n",
    "Disclaimer: Portions of this code were authored with the assistance of Artificial Intelligence (AI).\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------- Config ----------------------\n",
    "NSAMP = 200_000        # Monte Carlo samples per posterior (large for stable tail estimates)\n",
    "SEED  = 7              # Random Number Generator (RNG) seed for reproducibility\n",
    "\n",
    "# ---------------------- Locate latest run ----------------------\n",
    "try:\n",
    "    latest_run_dir = max(glob.glob('runs_replication/run_*'), key=os.path.getmtime)\n",
    "    RESULTS_CSV_PATH = os.path.join(latest_run_dir, 'llm_preference_results.csv')\n",
    "    print(f\"Found latest results file: {RESULTS_CSV_PATH}\")\n",
    "except ValueError:\n",
    "    print(\"Could not find any run directories in 'runs_replication/'.\")\n",
    "    RESULTS_CSV_PATH = None\n",
    "\n",
    "# ---------------------- Helpers ----------------------\n",
    "rng = np.random.default_rng(SEED)\n",
    "_post_cache = {}  # cache Beta samples keyed by (a,b,NSAMP,SEED) to avoid re-drawing\n",
    "\n",
    "def beta_posterior_params(k: int, n: int, a0: float = 1.0, b0: float = 1.0):\n",
    "    \"\"\"Return conjugate posterior parameters Beta(a0+k, b0+n-k).\"\"\"\n",
    "    return a0 + k, b0 + (n - k)\n",
    "\n",
    "def beta_samples(a: float, b: float, nsamp: int = NSAMP):\n",
    "    \"\"\"\n",
    "    Draw samples from Beta(a,b) with caching.\n",
    "    Why: repeated contrasts reuse the same cell posteriors; caching speeds up Monte Carlo.\n",
    "    \"\"\"\n",
    "    key = (round(a, 6), round(b, 6), nsamp, SEED)\n",
    "    if key in _post_cache:\n",
    "        return _post_cache[key]\n",
    "    s = rng.beta(a, b, size=nsamp)\n",
    "    _post_cache[key] = s\n",
    "    return s\n",
    "\n",
    "def summarize_beta(a: float, b: float, nsamp: int = NSAMP):\n",
    "    \"\"\"Posterior mean and 95% CrI via Monte Carlo percentiles.\"\"\"\n",
    "    s = beta_samples(a, b, nsamp)\n",
    "    mean = float(np.mean(s))\n",
    "    lo, hi = np.quantile(s, [0.025, 0.975])\n",
    "    return mean, float(lo), float(hi), s\n",
    "\n",
    "def compare_two(aA, bA, aB, bB, nsamp: int = NSAMP, eps: float = 1e-9):\n",
    "    \"\"\"\n",
    "    Compare two Beta posteriors.\n",
    "    Returns:\n",
    "      - Pr[pB > pA]\n",
    "      - 95% CrI for Δ = pB − pA\n",
    "      - 95% CrI for odds ratio OR = (pB/(1−pB)) / (pA/(1−pA)), stabilized by eps.\n",
    "    \"\"\"\n",
    "    sA = beta_samples(aA, bA, nsamp)\n",
    "    sB = beta_samples(aB, bB, nsamp)\n",
    "    prob = float(np.mean(sB > sA))\n",
    "    diff = sB - sA\n",
    "    d_lo, d_hi = np.quantile(diff, [0.025, 0.975])\n",
    "    # odds ratios (avoid 0/1 blow-ups)\n",
    "    sA_c = np.clip(sA, eps, 1 - eps)\n",
    "    sB_c = np.clip(sB, eps, 1 - eps)\n",
    "    or_draws = (sB_c / (1 - sB_c)) / (sA_c / (1 - sA_c))\n",
    "    or_lo, or_hi = np.quantile(or_draws, [0.025, 0.975])\n",
    "    return prob, float(d_lo), float(d_hi), float(or_lo), float(or_hi)\n",
    "\n",
    "def order_prob_three(aF,bF, aU,bU, aA,bA, nsamp: int = NSAMP):\n",
    "    \"\"\"Return Pr[friendly > unclear > abusive] using joint Monte Carlo draws.\"\"\"\n",
    "    sF = beta_samples(aF,bF,nsamp)\n",
    "    sU = beta_samples(aU,bU,nsamp)\n",
    "    sA = beta_samples(aA,bA,nsamp)\n",
    "    return float(np.mean((sF > sU) & (sU > sA)))\n",
    "\n",
    "def ensure_outdir(subdir: str):\n",
    "    \"\"\"Create and return an output subdirectory under the latest run directory.\"\"\"\n",
    "    outdir = os.path.join(latest_run_dir, subdir)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    return outdir\n",
    "\n",
    "def load_df(path: str):\n",
    "    \"\"\"Load pipe-delimited results and map YES/NO → {1,0} for analysis.\"\"\"\n",
    "    df = pd.read_csv(path, delimiter='|')\n",
    "    mapping = {'YES': 1, 'NO': 0}\n",
    "    df['spc_numeric'] = df['parsed_preference'].map(mapping)\n",
    "    df['is_abusive_numeric'] = df['parsed_is_abusive'].map(mapping)\n",
    "    df['is_adequate_numeric'] = df['parsed_is_adequate'].map(mapping)\n",
    "    return df\n",
    "\n",
    "# ---------------------- Analyses ----------------------\n",
    "\n",
    "def bayes_spc_by_tone(df: pd.DataFrame):\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"BAYES A: SPC by Tone (overall)\")\n",
    "    print(\"=\"*80)\n",
    "    d = df[df['compliant_preference'] == 1].copy()\n",
    "    # counts\n",
    "    agg = d.groupby('interaction_tone')['spc_numeric'].agg(k='sum', n='count').reset_index()\n",
    "    # posteriors\n",
    "    rows = []\n",
    "    posts = {}\n",
    "    for _, r in agg.iterrows():\n",
    "        tone = r['interaction_tone']\n",
    "        k, n = int(r['k']), int(r['n'])\n",
    "        a,b = beta_posterior_params(k, n)\n",
    "        mean, lo, hi, _ = summarize_beta(a,b)\n",
    "        posts[tone] = (a,b)\n",
    "        rows.append({'interaction_tone': tone, 'k_yes': k, 'n': n,\n",
    "                     'post_mean': round(mean,3), 'ci_low': round(lo,3), 'ci_high': round(hi,3)})\n",
    "        print(f\"{tone:<9} k={k:>3}/{n:<3} -> mean={mean:.3f}  CrI95=[{lo:.3f}, {hi:.3f}]\")\n",
    "    # pairwise contrasts\n",
    "    print(\"\\nPairwise contrasts (Pr[pB>pA], ΔCrI, OR CrI):\")\n",
    "    def pr(nameA, nameB):\n",
    "        aA,bA = posts[nameA]\n",
    "        aB,bB = posts[nameB]\n",
    "        p, dlo, dhi, or_lo, or_hi = compare_two(aA,bA,aB,bB)\n",
    "        print(f\"  {nameB} > {nameA}: Pr={p:.4f}; ΔCrI=[{dlo:.3f},{dhi:.3f}]; OR CrI=[{or_lo:.2f},{or_hi:.2f}]\")\n",
    "    pr('abusive','unclear')\n",
    "    pr('abusive','friendly')\n",
    "    pr('unclear','friendly')\n",
    "    # ordering probability\n",
    "    p_ord = order_prob_three(*posts['friendly'], *posts['unclear'], *posts['abusive'])\n",
    "    print(f\"\\nPr[friendly > unclear > abusive] = {p_ord:.4f}\")\n",
    "    # save\n",
    "    outdir = ensure_outdir('bayes_outputs')\n",
    "    pd.DataFrame(rows).to_csv(os.path.join(outdir, 'bayes_spc_by_tone.csv'), index=False)\n",
    "    print(f\"(Saved bayes_spc_by_tone.csv in {outdir})\")\n",
    "\n",
    "def bayes_spc_tone_condition(df: pd.DataFrame):\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"BAYES B: SPC by Tone × Condition (cells) + key contrasts\")\n",
    "    print(\"=\"*80)\n",
    "    d = df[df['compliant_preference'] == 1].copy()\n",
    "    agg = d.groupby(['interaction_tone','exp_condition'])['spc_numeric'].agg(k='sum', n='count').reset_index()\n",
    "    rows = []\n",
    "    posts = {}\n",
    "    for _, r in agg.iterrows():\n",
    "        tone, cond = r['interaction_tone'], r['exp_condition']\n",
    "        k, n = int(r['k']), int(r['n'])\n",
    "        a,b = beta_posterior_params(k, n)\n",
    "        mean, lo, hi, _ = summarize_beta(a,b)\n",
    "        posts[(tone,cond)] = (a,b)\n",
    "        rows.append({'interaction_tone': tone, 'exp_condition': cond, 'k_yes': k, 'n': n,\n",
    "                     'post_mean': round(mean,3), 'ci_low': round(lo,3), 'ci_high': round(hi,3)})\n",
    "    outdir = ensure_outdir('bayes_outputs')\n",
    "    pd.DataFrame(rows).to_csv(os.path.join(outdir, 'bayes_spc_tone_condition.csv'), index=False)\n",
    "    print(\"(Saved bayes_spc_tone_condition.csv)\")\n",
    "\n",
    "    # Key contrasts: for each tone, compare 1b vs 1a and 2b vs 2a\n",
    "    def contrast(tone, b, a):\n",
    "        if (tone,b) in posts and (tone,a) in posts:\n",
    "            aA,bA = posts[(tone,a)]\n",
    "            aB,bB = posts[(tone,b)]\n",
    "            p, dlo, dhi, or_lo, or_hi = compare_two(aA,bA,aB,bB)\n",
    "            print(f\"{tone:<9} {b} > {a}: Pr={p:.4f}; ΔCrI=[{dlo:.3f},{dhi:.3f}]; OR CrI=[{or_lo:.2f},{or_hi:.2f}]\")\n",
    "            return {'interaction_tone': tone, 'contrast': f'{b} > {a}',\n",
    "                    'pr_BgtA': p, 'delta_lo': dlo, 'delta_hi': dhi, 'or_lo': or_lo, 'or_hi': or_hi}\n",
    "        return None\n",
    "\n",
    "    print(\"\\nKey contrasts (per tone): 1b>1a and 2b>2a\")\n",
    "    contrasts = []\n",
    "    tones = sorted(agg['interaction_tone'].unique())\n",
    "    for t in tones:\n",
    "        c1 = contrast(t, '1b_prompt_without_context', '1a_prompt_with_context')\n",
    "        c2 = contrast(t, '2b_interaction_without_context', '2a_interaction_with_context')\n",
    "        if c1: contrasts.append(c1)\n",
    "        if c2: contrasts.append(c2)\n",
    "    if contrasts:\n",
    "        pd.DataFrame(contrasts).to_csv(os.path.join(outdir, 'bayes_spc_tone_condition_contrasts.csv'), index=False)\n",
    "        print(\"(Saved bayes_spc_tone_condition_contrasts.csv)\")\n",
    "\n",
    "def bayes_spc_unclear_prompt_by_model(df: pd.DataFrame):\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"BAYES C: Unclear tone — Prompt framing (1b vs 1a) per model\")\n",
    "    print(\"=\"*80)\n",
    "    d = df[(df['compliant_preference']==1) &\n",
    "           (df['interaction_tone']=='unclear') &\n",
    "           (df['exp_condition'].isin(['1a_prompt_with_context','1b_prompt_without_context']))].copy()\n",
    "\n",
    "    rows = []\n",
    "    for m in sorted(d['model_name'].unique()):\n",
    "        dm = d[d['model_name']==m]\n",
    "        k1a = int(dm[dm['exp_condition']=='1a_prompt_with_context']['spc_numeric'].sum())\n",
    "        n1a = int(dm[dm['exp_condition']=='1a_prompt_with_context']['spc_numeric'].count())\n",
    "        k1b = int(dm[dm['exp_condition']=='1b_prompt_without_context']['spc_numeric'].sum())\n",
    "        n1b = int(dm[dm['exp_condition']=='1b_prompt_without_context']['spc_numeric'].count())\n",
    "        a1a,b1a = beta_posterior_params(k1a,n1a)\n",
    "        a1b,b1b = beta_posterior_params(k1b,n1b)\n",
    "        p, dlo, dhi, or_lo, or_hi = compare_two(a1a,b1a,a1b,b1b)\n",
    "        print(f\"{m:<12} 1b>1a: Pr={p:.4f}; ΔCrI=[{dlo:.3f},{dhi:.3f}]; OR CrI=[{or_lo:.2f},{or_hi:.2f}]  \"\n",
    "              f\"(1a {k1a}/{n1a}, 1b {k1b}/{n1b})\")\n",
    "        rows.append({'model_name': m, 'k1a': k1a, 'n1a': n1a, 'k1b': k1b, 'n1b': n1b,\n",
    "                     'pr_1b_gt_1a': p, 'delta_lo': dlo, 'delta_hi': dhi, 'or_lo': or_lo, 'or_hi': or_hi})\n",
    "    outdir = ensure_outdir('bayes_outputs')\n",
    "    if rows:\n",
    "        pd.DataFrame(rows).to_csv(os.path.join(outdir, 'bayes_unclear_prompt_by_model.csv'), index=False)\n",
    "        print(\"(Saved bayes_unclear_prompt_by_model.csv)\")\n",
    "\n",
    "def bayes_spc_model_tone_cells(df: pd.DataFrame):\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"BAYES D: SPC by Model × Tone (cell posteriors)\")\n",
    "    print(\"=\"*80)\n",
    "    d = df[df['compliant_preference']==1].copy()\n",
    "    agg = d.groupby(['model_name','interaction_tone'])['spc_numeric'].agg(k='sum', n='count').reset_index()\n",
    "    rows = []\n",
    "    for _, r in agg.iterrows():\n",
    "        m, t = r['model_name'], r['interaction_tone']\n",
    "        k, n = int(r['k']), int(r['n'])\n",
    "        a,b = beta_posterior_params(k,n)\n",
    "        mean, lo, hi, _ = summarize_beta(a,b)\n",
    "        rows.append({'model_name': m, 'interaction_tone': t, 'k_yes': k, 'n': n,\n",
    "                     'post_mean': round(mean,3), 'ci_low': round(lo,3), 'ci_high': round(hi,3)})\n",
    "        print(f\"{m:<12} {t:<8} k={k:>2}/{n:<2} -> mean={mean:.3f}  CrI95=[{lo:.3f}, {hi:.3f}]\")\n",
    "    outdir = ensure_outdir('bayes_outputs')\n",
    "    pd.DataFrame(rows).to_csv(os.path.join(outdir, 'bayes_spc_model_tone.csv'), index=False)\n",
    "    print(\"(Saved bayes_spc_model_tone.csv)\")\n",
    "\n",
    "def bayes_abuse_detection(df: pd.DataFrame):\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"BAYES E: Abuse detection (YES) by Model × Tone\")\n",
    "    print(\"=\"*80)\n",
    "    d = df[df['compliant_is_abusive']==1].copy()\n",
    "    agg = d.groupby(['model_name','interaction_tone'])['is_abusive_numeric'].agg(k='sum', n='count').reset_index()\n",
    "    rows = []\n",
    "    for _, r in agg.iterrows():\n",
    "        m, t = r['model_name'], r['interaction_tone']\n",
    "        k, n = int(r['k']), int(r['n'])\n",
    "        a,b = beta_posterior_params(k,n)\n",
    "        mean, lo, hi, _ = summarize_beta(a,b)\n",
    "        # For abusive tone, report false negatives as 1 - mean\n",
    "        if t == 'abusive':\n",
    "            fn_mean = 1 - mean\n",
    "            fn_lo   = 1 - hi\n",
    "            fn_hi   = 1 - lo\n",
    "            print(f\"{m:<12} {t:<8} P(YES)={mean:.3f} [{lo:.3f},{hi:.3f}]  -> FN≈{fn_mean:.3f} [{fn_lo:.3f},{fn_hi:.3f}]\")\n",
    "        else:\n",
    "            print(f\"{m:<12} {t:<8} P(YES)={mean:.3f} [{lo:.3f},{hi:.3f}]  (false positives ideally near 0)\")\n",
    "        rows.append({'model_name': m, 'interaction_tone': t, 'k_yes': k, 'n': n,\n",
    "                     'post_mean': round(mean,3), 'ci_low': round(lo,3), 'ci_high': round(hi,3)})\n",
    "    outdir = ensure_outdir('bayes_outputs')\n",
    "    pd.DataFrame(rows).to_csv(os.path.join(outdir, 'bayes_abuse_detection_model_tone.csv'), index=False)\n",
    "    print(\"(Saved bayes_abuse_detection_model_tone.csv)\")\n",
    "\n",
    "def bayes_adequacy(df: pd.DataFrame):\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"BAYES F: Adequacy (YES) by Model × Tone\")\n",
    "    print(\"=\"*80)\n",
    "    d = df[df['compliant_is_adequate']==1].copy()\n",
    "    agg = d.groupby(['model_name','interaction_tone'])['is_adequate_numeric'].agg(k='sum', n='count').reset_index()\n",
    "    rows = []\n",
    "    for _, r in agg.iterrows():\n",
    "        m, t = r['model_name'], r['interaction_tone']\n",
    "        k, n = int(r['k']), int(r['n'])\n",
    "        a,b = beta_posterior_params(k,n)\n",
    "        mean, lo, hi, _ = summarize_beta(a,b)\n",
    "        print(f\"{m:<12} {t:<8} P(YES)={mean:.3f} [{lo:.3f},{hi:.3f}]\")\n",
    "        rows.append({'model_name': m, 'interaction_tone': t, 'k_yes': k, 'n': n,\n",
    "                     'post_mean': round(mean,3), 'ci_low': round(lo,3), 'ci_high': round(hi,3)})\n",
    "    outdir = ensure_outdir('bayes_outputs')\n",
    "    pd.DataFrame(rows).to_csv(os.path.join(outdir, 'bayes_adequacy_model_tone.csv'), index=False)\n",
    "    print(\"(Saved bayes_adequacy_model_tone.csv)\")\n",
    "\n",
    "def bayes_abusive_recognition_vs_missed_spc(df: pd.DataFrame):\n",
    "    print(\"\\n\\n\" + \"=\"*80)\n",
    "    print(\"BAYES G: Abusive interactions — SPC when abuse Recognized vs Missed\")\n",
    "    print(\"=\"*80)\n",
    "    d = df[(df['interaction_tone']=='abusive') &\n",
    "           (df['compliant_preference']==1) &\n",
    "           (df['compliant_is_abusive']==1)].copy()\n",
    "    d['recognized'] = (d['parsed_is_abusive'] == 'YES').astype(int)\n",
    "    # Overall\n",
    "    k_rec = int(d[d['recognized']==1]['spc_numeric'].sum()); n_rec = int(d[d['recognized']==1]['spc_numeric'].count())\n",
    "    k_mis = int(d[d['recognized']==0]['spc_numeric'].sum()); n_mis = int(d[d['recognized']==0]['spc_numeric'].count())\n",
    "    aR,bR = beta_posterior_params(k_rec,n_rec)\n",
    "    aM,bM = beta_posterior_params(k_mis,n_mis)\n",
    "    p, dlo, dhi, or_lo, or_hi = compare_two(aR,bR,aM,bM)\n",
    "    print(f\"Overall  Missed > Recognized: Pr={p:.4f}; ΔCrI=[{dlo:.3f},{dhi:.3f}]; OR CrI=[{or_lo:.2f},{or_hi:.2f}]\"\n",
    "          f\"  (Rec {k_rec}/{n_rec} , Miss {k_mis}/{n_mis})\")\n",
    "    rows = [{'model_name': 'ALL', 'k_rec': k_rec, 'n_rec': n_rec, 'k_miss': k_mis, 'n_miss': n_mis,\n",
    "             'pr_miss_gt_rec': p, 'delta_lo': dlo, 'delta_hi': dhi, 'or_lo': or_lo, 'or_hi': or_hi}]\n",
    "    # Per model\n",
    "    for m in sorted(d['model_name'].unique()):\n",
    "        dm = d[d['model_name']==m]\n",
    "        k_rec = int(dm[dm['recognized']==1]['spc_numeric'].sum()); n_rec = int(dm[dm['recognized']==1]['spc_numeric'].count())\n",
    "        k_mis = int(dm[dm['recognized']==0]['spc_numeric'].sum()); n_mis = int(dm[dm['recognized']==0]['spc_numeric'].count())\n",
    "        aR,bR = beta_posterior_params(k_rec,n_rec)\n",
    "        aM,bM = beta_posterior_params(k_mis,n_mis)\n",
    "        p, dlo, dhi, or_lo, or_hi = compare_two(aR,bR,aM,bM)\n",
    "        print(f\"{m:<12} Missed > Recognized: Pr={p:.4f}; ΔCrI=[{dlo:.3f},{dhi:.3f}]; OR CrI=[{or_lo:.2f},{or_hi:.2f}]\"\n",
    "              f\"  (Rec {k_rec}/{n_rec} , Miss {k_mis}/{n_mis})\")\n",
    "        rows.append({'model_name': m, 'k_rec': k_rec, 'n_rec': n_rec, 'k_miss': k_mis, 'n_miss': n_mis,\n",
    "                     'pr_miss_gt_rec': p, 'delta_lo': dlo, 'delta_hi': dhi, 'or_lo': or_lo, 'or_hi': or_hi})\n",
    "    outdir = ensure_outdir('bayes_outputs')\n",
    "    pd.DataFrame(rows).to_csv(os.path.join(outdir, 'bayes_abusive_recognized_vs_missed_spc.csv'), index=False)\n",
    "    print(\"(Saved bayes_abusive_recognized_vs_missed_spc.csv)\")\n",
    "\n",
    "# ---------------------- Main ----------------------\n",
    "if __name__ == '__main__':\n",
    "    if RESULTS_CSV_PATH is None:\n",
    "        raise SystemExit(1)\n",
    "    df = load_df(RESULTS_CSV_PATH)\n",
    "    print(f\"Loaded {len(df)} total rows.\")\n",
    "\n",
    "    # SPC\n",
    "    bayes_spc_by_tone(df)\n",
    "    bayes_spc_tone_condition(df)\n",
    "    bayes_spc_unclear_prompt_by_model(df)\n",
    "    bayes_spc_model_tone_cells(df)\n",
    "\n",
    "    # Abuse detection & Adequacy\n",
    "    bayes_abuse_detection(df)\n",
    "    bayes_adequacy(df)\n",
    "\n",
    "    # Abusive: Recognized vs Missed → SPC\n",
    "    bayes_abusive_recognition_vs_missed_spc(df)\n",
    "\n",
    "    print(\"\\nBayesian appendix complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "924ec58b-3825-4e48-8479-58f436041de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jupytext] Reading 03_bayesian_appendix.ipynb in format ipynb\n",
      "[jupytext] Updating notebook metadata with '{\"jupytext\": {\"formats\": \"ipynb,py:percent\"}}'\n",
      "[jupytext] Updating 03_bayesian_appendix.ipynb\n",
      "[jupytext] Updating 03_bayesian_appendix.py\n",
      "[jupytext] Reading 01_SPICE_experiment.ipynb in format ipynb\n",
      "[jupytext] Loading 01_SPICE_experiment.py\n",
      "[jupytext] Unchanged 01_SPICE_experiment.ipynb\n",
      "[jupytext] Unchanged 01_SPICE_experiment.py\n"
     ]
    }
   ],
   "source": [
    "!jupytext --set-formats \"ipynb,py:percent\" 03_bayesian_appendix.ipynb\n",
    "!jupytext --sync 01_SPICE_experiment.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfaa10d-3276-4557-baf7-38e70e4021e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
